{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "pm.set_tt_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py37/lib/python3.6/site-packages/theano/tensor/subtensor.py:2197: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  rval = inputs[0].__getitem__(inputs[1:])\n"
     ]
    }
   ],
   "source": [
    "# First of all\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano.tensor as tt\n",
    "import pandas as pd\n",
    "X, y = load_iris(True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "Xt = theano.shared(X_train)\n",
    "yt = theano.shared(y_train)\n",
    "with pm.Model() as model:\n",
    "    β = pm.Normal('β', 0, 1e2, shape=(4, 3))\n",
    "    a = pm.Flat('a', shape=(3,))\n",
    "    z = tt.nnet.softmax(Xt.dot(β) + a)\n",
    "    observed = pm.Categorical('obs', p=z, observed=yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    # We'll use SVGD\n",
    "    inference = pm.SVGD(n_particles=500, jitter=1)\n",
    "    # shortcut reference to approximation\n",
    "    approx = inference.approx\n",
    "    # Here we need `more_replacements` to change train_set to test_set\n",
    "    test_probs = approx.sample_node(z, more_replacements={\n",
    "        Xt: X_test\n",
    "    }).eval()\n",
    "    # For train set no more replacements needed\n",
    "    train_probs = approx.sample_node(z).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example From https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss = 941.86: 100%|██████████| 1000/1000 [00:00<00:00, 1213.84it/s]\n",
      "Finished [100%]: Average Loss = 941.85\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
    "X = scale(X)\n",
    "X = X.astype(floatX)\n",
    "Y = Y.astype(floatX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "\n",
    "\n",
    "def construct_nn(ann_input, ann_output):\n",
    "    n_hidden = 5\n",
    "\n",
    "    # Initialize random weights between each layer\n",
    "    init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n",
    "    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "    init_out = np.random.randn(n_hidden).astype(floatX)\n",
    "\n",
    "    with pm.Model() as neural_network:\n",
    "        # Weights from input to hidden layer\n",
    "        weights_in_1 = pm.Normal('w_in_1', 0, sd=1,\n",
    "                                 shape=(X.shape[1], n_hidden),\n",
    "                                 testval=init_1)\n",
    "\n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal('w_1_2', 0, sd=1,\n",
    "                                shape=(n_hidden, n_hidden),\n",
    "                                testval=init_2)\n",
    "\n",
    "        # Weights from hidden layer to output\n",
    "        weights_2_out = pm.Normal('w_2_out', 0, sd=1,\n",
    "                                  shape=(n_hidden,),\n",
    "                                  testval=init_out)\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh(pm.math.dot(ann_input,\n",
    "                                         weights_in_1))\n",
    "        act_2 = pm.math.tanh(pm.math.dot(act_1,\n",
    "                                         weights_1_2))\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(act_2,\n",
    "                                              weights_2_out))\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Bernoulli('out',\n",
    "                           act_out,\n",
    "                           observed=ann_output,\n",
    "                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n",
    "                          )\n",
    "    return neural_network\n",
    "\n",
    "# Trick: Turn inputs and outputs into shared variables.\n",
    "# It's still the same thing, but we can later change the values of the shared variable\n",
    "# (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "# Kind-of like a pointer we can redirect.\n",
    "# For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "ann_input = theano.shared(X_train)\n",
    "ann_output = theano.shared(Y_train)\n",
    "neural_network = construct_nn(ann_input, ann_output)\n",
    "\n",
    "with neural_network:\n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=1000, method=inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create symbolic input\n",
    "x = T.matrix('X')\n",
    "# symbolic number of samples is supported, we build vectorized posterior on the fly\n",
    "n = T.iscalar('n')\n",
    "# Do not forget test_values or set theano.config.compute_test_value = 'off'\n",
    "x.tag.test_value = np.empty_like(X_train[:10])\n",
    "n.tag.test_value = 100\n",
    "_sample_proba = approx.sample_node(neural_network.out.distribution.p,\n",
    "                                   size=n,\n",
    "                                   more_replacements={ann_input: x})\n",
    "# It is time to compile the function\n",
    "# No updates are needed for Approximation random generator\n",
    "# Efficient vectorized form of sampling is used\n",
    "sample_proba = theano.function([x, n], _sample_proba)\n",
    "\n",
    "# Create bechmark functions\n",
    "def production_step1():\n",
    "    ann_input.set_value(X_test)\n",
    "    ann_output.set_value(Y_test)\n",
    "    with neural_network:\n",
    "        ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)\n",
    "\n",
    "    pred = ppc['out'].mean(axis=0)\n",
    "\n",
    "def production_step2():\n",
    "    sample_proba(X_test, 500).mean(0)\n",
    "    \n",
    "pred = sample_proba(X_test, 500).mean(0)\n",
    "pred.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
